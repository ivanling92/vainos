{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VainosAgri_Train.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMMONpwozLFQ0s9+2s5xwiQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ivanling92/vainos/blob/master/VainosAgri_Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VAINOS AGRI Plant Image classification with Tensorflow\n",
        "Copyright 2022 Dr Ivan Ling\n",
        " - Adapted from Google Tensorflow, Google Drive Integration and PIL library"
      ],
      "metadata": {
        "id": "blORrq0ySqDQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libraries\n"
      ],
      "metadata": {
        "id": "Sq9y-Cn2MQ40"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDjFGwJVMLN7"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the Zip file to google drive\n",
        "\n",
        "Use the Google Drive library obtain training files from drive:\n",
        "\n"
      ],
      "metadata": {
        "id": "B51saWuvMm62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "\n",
        "#Get data from Google Drive\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#Set the data path\n",
        "fid = drive.ListFile({'q':\"title='flower_photos.zip'\"}).GetList()[0]['id']\n",
        "f = drive.CreateFile({'id': fid})\n",
        "f.GetContentFile('flower_photos.zip')"
      ],
      "metadata": {
        "id": "jkPSoVxkaV3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Unzip the file\n",
        "!unzip flower_photos.zip"
      ],
      "metadata": {
        "id": "7v-kpJeLaziU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import to tensorflow\n",
        "import pathlib\n",
        "data_dir = '/content/flower_photos'"
      ],
      "metadata": {
        "id": "scmG6MUYMokJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "List the number of image loaded. Note that pathlib.Path() was used to get access to a lot of useful directory functions, such as glob()\n",
        "\n",
        "The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell."
      ],
      "metadata": {
        "id": "-HOCCiMMMww6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make the directory a Path object\n",
        "data_dir = pathlib.Path(data_dir)\n",
        "\n",
        "#Use the Path glob function to find pathname that matches \".jpg\". Use this to get the number of image inside the folder. \n",
        "image_count = len(list(data_dir.glob('*/*.jpg')))\n",
        "print(image_count)"
      ],
      "metadata": {
        "id": "BhLwyXwedhQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at the content of the downloaded file, we write a for loop to print everything out."
      ],
      "metadata": {
        "id": "uompg5HwgZsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for filename in list(data_dir.glob(\"*/\")):\n",
        "  print(filename)"
      ],
      "metadata": {
        "id": "n2I5bCydgYxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To look at the images:"
      ],
      "metadata": {
        "id": "4eW9Nmv-gf8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roses = list(data_dir.glob('roses/*'))\n",
        "tulips = list(data_dir.glob('tulips/*'))\n",
        "\n",
        "\n",
        "\n",
        "PIL.Image.open(str(tulips[56]))"
      ],
      "metadata": {
        "id": "lEw--_5eeJ4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating the model\n"
      ],
      "metadata": {
        "id": "8k4Bg7QLg3tZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H74l2DoDI2XD"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "img_height = 180\n",
        "img_width = 180"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training and Testing dataset\n",
        "\n",
        "Create 2 data sets, one for training, and one for testing (validating). "
      ],
      "metadata": {
        "id": "3cTNoqdHhYp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "w0kghnVchhwv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHAxkHX5JD3k"
      },
      "outputs": [],
      "source": [
        "class_names = train_ds.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize data"
      ],
      "metadata": {
        "id": "_WGGewvlou9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "V58puE0Uowp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Makes training run faster like Naruto's dad"
      ],
      "metadata": {
        "id": "bicj2z_4pFh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "f85vAFiwpPtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Performance optimization. Don't ask me why it works.... it works.\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "KHhUwk81pbzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Standardize the data\n",
        "\n",
        "The original image data is from 0 to 255. We need to make it standardized to 0-1.0"
      ],
      "metadata": {
        "id": "8NJQNIn-pmnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "normalization_layer = layers.Rescaling(1./255)"
      ],
      "metadata": {
        "id": "XUYlUMK_pxAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create the model"
      ],
      "metadata": {
        "id": "BefYAlxyqC7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(class_names)\n",
        "\n",
        "\n",
        "#Generate more training data\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "    layers.RandomFlip(\"horizontal\",\n",
        "                      input_shape=(img_height,\n",
        "                                  img_width,\n",
        "                                  3)),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "  ]\n",
        ")\n",
        "\n",
        "model = Sequential([\n",
        "  data_augmentation,\n",
        "  normalization_layer,\n",
        "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Dropout(0.2),\n",
        "  layers.Flatten(),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(num_classes)\n",
        "])\n",
        "\n",
        "#Compile model\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "#Summary\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vaAM99HYqE6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##View augmented data\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, _ in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "PmEHF6u7qzg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train model"
      ],
      "metadata": {
        "id": "4MWo_Q8-qWtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs=10\n",
        "history = model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  epochs=epochs\n",
        ")"
      ],
      "metadata": {
        "id": "skpu469CqYY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualize Result"
      ],
      "metadata": {
        "id": "9ULRbUszqOZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ADwh0pTUqRQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing"
      ],
      "metadata": {
        "id": "xamsCnwlskDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sunflower_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/592px-Red_sunflower.jpg\"\n",
        "sunflower_path = tf.keras.utils.get_file('Red_sunflower', origin=sunflower_url)\n",
        "\n",
        "img = tf.keras.utils.load_img(\n",
        "    sunflower_path, target_size=(img_height, img_width)\n",
        ")\n",
        "img_array = tf.keras.utils.img_to_array(img)\n",
        "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
        "\n",
        "predictions = model.predict(img_array)\n",
        "score = tf.nn.softmax(predictions[0])\n",
        "\n",
        "PIL.Image.open(img)\n",
        "\n",
        "print(\n",
        "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
        "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
        ")"
      ],
      "metadata": {
        "id": "kmoyj4pwsh6x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}